%!TEX root = apunte_estadistica.tex


\chapter{Test de Hipótesis}


\clase{Clase 13: 24/9}
\section{Teoría de decisiones}
\label{sec:teoría_de_decisiones}

En términos generales, la teoría de decisiones estudia las acciones que puede tomar un agente en un escenario dado. En este contexto afloran de forma natural los conceptos de incertibumbre (de aspectos clave del escenario), funciones de pérdida y procedimientos de decisión. En estadística, podemos identificar al menos los siguientes problemas de decisión.


\begin{itemize}
	\item \textbf{Estimación}: Decidir el valor apropiado para un parámetro desconocido usando datos $X$ y un distribución condicional $P_\theta$
	\item \textbf{Test}: Decidir la hipótesis correcta usando datos $X\sim P_\theta$
	\begin{align}
		H_0: P_\theta\in\cP_0\\
		H_1: P_\theta\not\in\cP_0
	\end{align}
	\item \textbf{Ranking}: Elaborar una lista ordenada de ítems, por ejemplo, productos evaluados por una muestra de la población, resultados de eventos deportivos o juegos online. 
	\item \textbf{Predicción}: Estimar/decidir el valor de una variable dependiente en base a observaciones de observaciones pasadas. 
\end{itemize}

Como se puede apreciar, la teoría de decisiones presenta un contexto general para abordar una gran cantidad de situaciones. A continuación se describen los elementos básicos de un problema de decisión, en donde, con fines ilustrativos, ponemos como ejemplo su contraparte en el problema de estimación.

\begin{itemize}
	\item $\Theta = \{\theta\}$ es el espacio de estado, donde la cantidad $\theta$ es el \textit{estado del mundo}. En el problema de estimación, donde convenientemente se ha usado la misma notación, $\theta$ es el parámetro del modelo
	\item $\cA=\{a\}$ es el espacio de acciones, donde $a$ es la acción a tomar por el estadístico. En estimación, podemos abusar de la notación y decir que la acción $a$ es elegir el valor $a$ para el parámetro $\theta$. 
	\item $L(\theta,a)$ es la función de pérdida asociada a tomar la decisión $a$ cuando el estado es $\theta$; nótese que $L:\Theta\times\cA\rightarrow\R$. En el caso de estimación, usualmente consideramos la pérdida cuadrática:
	\begin{equation}
		L(\theta,a) = (\theta-a)^2.
	\end{equation}
\end{itemize}

\begin{example}(Inversión bajo incertidumbre)
	Consideremos los estados $\Theta = \{\theta_1,\theta_2\}$, donde $\theta_1$ quiere decir mercado sano y $\theta_2$ quiere decir mercado no sano. Se debe elegir una estrategia de inversión del siguiente conjunto $\cA=\{a_1,a_2,a_3,a_4,a_5,a_6\}$ con la siguiente función de costo $L(\theta,a)$
	\begin{table}[h]
		\centering
		\begin{tabular}{c|ccccc}
			$L(\theta,a)$   & $a_1$ & $a_2$ & $a_3$ & $a_4$ & $a_5$ \\
			\hline
			$\theta_1$ & -4   & -4   & -1   & 2    & 4    \\
			$\theta_2$  & 4    & 0    & -1   & -6   & -4
		\end{tabular}
	\end{table} 
	Notemos que \begin{itemize}
		\item $a_1$ es bueno  cuando $\theta=\theta_1$
		\item $a_2$ es bueno cuando $\theta=\theta_2$
		\item $a_3$ es medianamente bueno (pérdida negativa) siempre
	\end{itemize}
	entonces, ¿cómo elegimos la acción?
\end{example}

Además de los elementos básicos del problema de decisión (estado, acciones y pérdida), en el enfoque estadístico de la teoría de decisiones existen los siguientes elementos:
\begin{itemize}
	\item $X\sim P_\theta$ es la variable aleatoria, la cual define la distribución condicional, el espacio muestral, la densidad, etc. 
	\item $\delta(X)$ es el procedimiento de la decisión, es decir, el mapa que asocia una observación $X=x$ con la acción $a$:
	\begin{equation}
		\delta(\cdot):\cX\rightarrow\cA.
	\end{equation}
	\item $\cD=\{\delta:\cX\rightarrow\cA\}$ es el espacio de decisiones
	\item $R(\theta,\delta)$ es el riesgo asociado a $\delta$ y $\theta$, el cual está definido como el valor esperado de la pérdida incurrida al tomar la acción $\delta(X)$ cuando el parámetro es $\theta$. Es decir, 
	\begin{equation}
	 	R(\theta,\delta) = \Et{L(\theta,\delta(X))}.
	 \end{equation} 
\end{itemize}


\begin{example}
	Volviendo al contexto del problema de estimación, consideremos el caso en que usando una VA $X\sim\cN(\theta,1), \theta\in\R$ debemos encontrar el valor de $\theta$. En el contexto de teoría estadística de decisiones, el espacio de posibles acciones es precisamente en espacio de parámetros, es decir, 
	\begin{equation}
		\cA = \Theta = \R.
	\end{equation}
	Elegimos además la pérdida cuadrática, $L(\theta,\thetahat(X)) = (\theta-\thetahat(X))^2$, asociada a elegir $\thetahat(X)$ como el valor del parámetro $\theta$. Consideremos que el espacio de acciones está dado por versiones escaladas de la observación $X$, es decir, 
	\begin{equation}
		\cA = \{cX, c\in[0,1]\}.
	\end{equation}
	Con esta forma del estimador, podemos calcular el riesgo asociado mediante  
	\begin{equation}
		R(\theta,\thetahat)= \Et{(\thetahat(X)-\theta)^2} = \Vt{\thetahat(X)} + \Et{\thetahat(X)-\theta}^2 = c^2 + (c-1)^2\theta^2
	\end{equation}
	¿Qué valor de $c$ sugiere elegir?
\end{example}


\section{Intuición en un test de hipótesis} 
\label{sec:int_hipótesis}

El objetivo del análisis estadístico es obtener conclusiones razonables mediante el uso de observaciones, como también aseveraciones precisas sobre la incertidumbre asociada a dichas conclusiones. De forma ilustrativa, consideremos el siguiente escenario hipotético.

En base a estudios preliminares, se sabe que los pesos de los recién nacidos (RN) en Santiago, Chile, distribuyen aproximadamente normal con promedio 3000gr y desviación estándar de 500gr. Creemos que los RNs en Osorno pesan, en promedio, más que los RNs en Santiago. Nos gustaría formalmente aceptar o rechazar esta hipótesis. 

Intuitivamente, una forma de evaluar esta hipótesis es tomar una muestra de RNs en Osorno, calcular su peso promedio y verificar si éste es \textit{significativamente mayor} que 3000gr. Asumamos que hemos tenido acceso al peso de 50 RNs nacidos en Osorno, los cuales exhiben un preso promedio de 3200gr. ¿Podemos entonces concluir directamente y decir que efectivamente los RNs de Osorno pesan más que los de Santiago?  Si bien esta es una posibilidad, una postura más escéptica podría argumentar que el obtener una población de 50 RNs con peso promedio de 3200gr es perfectamente plausible de una población de RNs distribuidos de acuerdo a $\cN(3000,500^2)$. Entonces, ¿cómo justificamos la plausibilidad de este resultado?

Para esto distingamos entre las dos hipótesis: 

\begin{itemize}
	\item $H_1$: Los RNs en Osorno pesan en promedio más de 3000gr (esta es la hipótesis alternativa)
	\item $H_0$: Los RNs en Osorno pesan en promedio 3000gr (esta es la hipótesis nula)
\end{itemize}

Para decidir cuál es verdadera, trataremos de \emph{falsificar} $H_0$. La forma de hacer esto es calcular la probabilidad de obtener el resultado observado bajo el supuesto que $H_0$ es cierta. En este caso, sabemos que una muestra 
\begin{equation}
	X = X_1,\ldots,X_{50}\sim\cN(3000,500^2),
\end{equation}
tiene una media que está distribuida de acuerdo a la siguiente densidad 
\begin{equation}
	\bar{X} = \frac{1}{50} \sum_{i=1}^{50} X_i \sim \cN(3000,500^2/50).
\end{equation}
Entonces, cuál es la probabilidad de que la muestra obtenida, $\bar{X}=3200$, haya sido generada por la distribución anterior? Para calcular esto, construyamos el \textbf{pivote} 
\begin{equation}
	z = \frac{\bar{X}-3000}{500/\sqrt{50}}\sim\cN(0,1)
\end{equation}
con la cual podemos realizar el cálculo:
\begin{equation}
	\Prob{\bar{X}\geq 3200} = \Prob{z = \frac{\bar{X}-3000}{500/\sqrt{50}}\geq 2\sqrt{2}} = 0.0023388674905235884,
\end{equation}
donde el valor de esta probabilidad puede ser calculado usando la función\footnote{Acrónimo de \textit{cumulative denstiy function}.} \texttt{cdf} de SciPy mediante la siguiente instrucción. 
\begin{lstlisting}[language=Python]
	from scipy.stats import norm
	import numpy as np
	print(1-norm.cdf(2*np.sqrt(2)))
\end{lstlisting}
Concluimos entonces que la probabilidad de que una muestra de 50 RNs exhiban un promedio de peso mayor o igual a 3200gr, bajo el supuesto que $H_0$ es cierta, es del orden del 0.23\%. 

Nos referiremos a esta probabilidad como \textbf{p-valor}, el cual nos dice cuán verosímil es que obtener la observación dada bajo el supuesto que la hipótesis nula es cierta. Mientras más pequeño es el p-valor, entonces más fuerte es la evidencia en contra de $H_0$. Entonces nos encontramos ante dos posibles explicaciones: 
\begin{itemize}
 	\item $H_0$ es falsa
 	\item hemos obtenido un resultado que solo ocurre una de cada 500 veces. 
 \end{itemize} 

\todo[inline]{Agregar comentario sobre doble negación y falsificación}

En cuando al umbral en el cual rechazamos $H_0$, nos referiremos a significancia del test $\alpha$ al umbral para el p-valor en el cual se rechaza el test. En general, este umbral es del 1\% o del 5\%, sin embargo esto depende de la aplicación en cuestión. Por ejemplo, si estamos considerando la administración de una droga que puede tener consecuencias fatales, entonces necesariamente nuestro nivel de significancia sea muy bajo, lo que quiere decir que la hipótesis nula requiere mucha evidencia en su contra para ser rechazada. Notemos que hay dos tipos de errores: El error de Tipo I en el cual $H_0$ es rechazada a pesar de que es verdadera y el error de Tipo II, donde $H_0$ no es rechazada a pesar de que es falsa (lo cual diremos que tiene probabilidad $\beta$). Los tipos de errores se definen mediante la siguiente tabla:

\vspace{1em}
\begin{center}
	\begin{tabular}{c|cc}
			  & $H_0$ es cierto & $H_0$ no es cierto  \\
			\hline
			se rechaza $H_0$  & error Tipo I  & no hay error    \\
			no se rechaza $H_0$  & no hay error   & error Tipo II
	\end{tabular}
\end{center}

Volviendo a nuestro ejemplo de los recién nacidos, el p-valor del test es del orden de 0.0023, lo cual, si consideramos una significancia del $\alpha=0.01=1\%$, resulta en el rechazo de $H_0$. Decimos entonces que \textbf{ hay suficiente evidencia para rechazar $H_0$ al 1\%}, o bien que \textbf{ rechazamos la hipótesis nula $H_0$ al 1\%}. Por el contrario, en el caso que el p-valor fuese mayor que el nivel de significancia del test, entonces no rechazamos $H_0$ y simplemente decimos que \textbf{ la evidencia para rechazar $H_0$ no es significativa al 1\%}
\clase{clase 14: 26/9}

\begin{tcolorbox}[title=Test de Hipótesis]
En resumen,  un test de hipótesis consta de los siguientes pasos: 

\begin{enumerate}
	\item Proponer una hipótesis alternativa $H_1$
	\item construir una hipótesis nula (básicamente lo contrario de $H_0$)
	\item Recolectar datos
	\item Calcular el pivote (un estadístico de prueba) usando los datos
	\item Calcular el p-valor para el pivote
	\item Comparar el p-valor con la significancia estadística. 
	\item Rechazar si corresponde
\end{enumerate}
\end{tcolorbox}

\textbf{Sobre p-valor y región crítica.}
Otra forma de cuantificar la evidencia en contra de $H_0$ es mediante la identificación de una región crítica, es decir, un subconjunto de $\cX$ en donde, de tomar valores la observación (o el estadístico) , su p-valor estaría por debajo del nivel de significancia y consecuentemente $H_0$ se rechazaría. En el ejemplo anterior, este puede ser calculado usando la función de SciPy \texttt{ppf}\footnote{Acrónimo para \emph{percent point function}.}. Considerando una significancia del 1\% podemos ejecutar
\begin{lstlisting}[language=Python]
	from scipy.stats import norm
	print(norm.ppf(0.99))
\end{lstlisting}
lo cual nos da una región crítica $[2.326,\infty)$, la cual contiene a nuestro p-valor $2\sqrt{2} = 2.82$; concluimos de igual forma y rechazamos $H_0$ al 1\%. \todo[inline]{Falta agregar gráfico ilustrando el uso de p-valor, pivote, significancia y región crítica.}

\todo[inline]{Falta discusión sobre test simétricos y asimétricos: gráfico ilustrando el uso de p-valor, pivote, significancia y región crítica.}



\section{Rechazo, potencia y nivel} 
\label{sec:def_hipótesis}

Formalmente, frente a dos hipótesis generales denotadas por 
\begin{align}
	H_0&: \theta\in\Theta_0\\
	H_1&: \theta\in\Theta_1,
\end{align}
definiremos el problema del test de hipótesis como la búsqueda de una funció
\begin{equation}
	\phi:\cX\rightarrow \{0,1\},
\end{equation}
donde:
\begin{itemize}
	\item Si $\phi(x)=0$ entonces aceptamos $H_0$ (no rechazamos $H_0$).
	\item Si $\phi(x)=1$ entonces rechazamos $H_0$, lo cual implícitamente acepta $H_1$. 
\end{itemize}
En teoría de decisiones, diríamos que $\phi$ es una regla de decisión. 

A continuación, revisamos definiciones que serán de utilidad para analizar y construir tests. 

\begin{definition}[Región crítica de un test]
	La región crítica o región de rechazo de un test de hipótesis $\phi$ se define como 
	\begin{equation}
		R_\phi = \{x\in\cX | \phi(x)=1\} = \phi^{-1}(1).
	\end{equation}
	
\end{definition}



\begin{definition}[Función de probabilidad de rechazo]
Para un test $\phi$ y cualquier parámetro $\theta\in\Theta$ podemos definir la probabilidad de rechazo mediante
\begin{equation}
 	\alpha_\phi(\theta) = \Probt{\phi(x)=1} = \Probt{x\in R_\phi}, \forall\theta\in\Theta,
 \end{equation}
donde nos gustaría entonces que $\alpha\approx 0$ si $\theta\in\Theta_0$ es cierto y que $\alpha\approx 1$ si $\theta\in\Theta_1$. Luego, usaremos esta función para evaluar la calidad del test.
\end{definition}

\begin{definition}[Potencia de un test]
En el caso que $H_1$ sea cierta, es decir, $\theta\in\Theta_1$, podemos definir la potencia del test como la probabilidad rechazar $H_0$ cuando $H_1$ es efectivamente cierta ($\theta\in\Theta_1$). Es decir,
\begin{equation}
 	\pi_\phi(\theta) = \Prob{\text{rechazar } H_0 | H_1 \text{ es cierta}}  = \Probtu{\phi(x)=1}
 \end{equation}
 \end{definition}
Nos gustaría entonces minimizar $\alpha(\theta)$ cuando $H_0$ y maximizar $\alpha(\theta)$ cuando $H_1$, lo cual es equivalente a minimizar la probabilidad de cometer errores de Tipo I y II respectivamente. 


\begin{example}[Un test absurdo]
	Existen tests absurdos, por ejemplo $\phi(x) = 0,\forall x\in\cX$. Este test tiene $\alpha(\theta)=0$ cuando $H_0$ (lo cual es bueno), por también tiene potencia nula, es decir, incluso si $H_1$, no rechaza a $H_0$. 
 \end{example} 
 En general, consideramos más importante prevenir un error de tipo I que uno de tipo II. 

 \begin{definition}[Nivel de un test]
 Decimos que un test es de nivel $\alpha\in[0,1]$ si 
\begin{equation}
 		\alpha_\phi(\theta)\leq\alpha, \forall \theta\in\Theta_0,
 	\end{equation}
 	equivalentemente, $\sup_{\theta\in\Theta_0}\alpha_\phi(\theta)\leq\alpha$. Además, denotamos por $T_\alpha$ la clase de todos los tests de nivel $\alpha$. 
 \end{definition}
 Dentro de esta clase, la cual nos restringe únicamente a los test que tienen probabilidad de rechazo acotada superiormente por $\alpha$  para $\theta\in\Theta_0$ (probabilidad de cometer error tipo I), podemos buscar el test de mayor potencia (probabilidad de rechazar $H_0$ cuando $H_1$ es cierta). Caracterizamos este test mediante: 

 \begin{definition}[Test uniformemente más potente, UMP]
 	Diremos que $\phi^\star$ es un test UMP (de nivel $\alpha$)  si 
 	\begin{equation}
 		\pi_{\phi^\star}(\theta)\geq \pi_{\phi}(\theta), \forall\theta\in\Theta_1.
 	\end{equation}
 	
 \end{definition}

 \todo[inline]{Falta definición/discusión sobre tests simples y compuestos.}

\clase{clase 15: 1/10}

\section{Test de Neyman-Pearson} 
\label{sub:test_de_neyman_pearson}
Consideremos el siguiente problema de test de dos hipótesis simples. 
\begin{equation}
	H_0:\theta\in\Theta_0=\{\theta_0\}\quad \text{v.s.}\quad H_1:\theta\in\Theta_1=\{\theta_1\},
\end{equation}
donde por una notación más simple escribiremos simplemente 
\begin{equation}
	H_0:\theta =\theta_0\quad \text{v.s.}\quad H_1:\theta = \theta_1,		
\end{equation}
y asumiremos que $\familiaparametrica=\{P_{\theta_0},P_{\theta_1}\}$ con densidades respectivamente dadas Por $p_0(x) = p_{\theta_0}(x)$ y $p_1(x) = p_{\theta_1}(x)$.

Denotamos además la región crítica (donde se rechaza $H_0$) mediante
\begin{equation}
	\label{eq:R_NR}
	R^* = \{x\in\cX | p_1(x) \geq k p_0(x)\},
\end{equation}
donde $k\in\R_+$ es una constante a determinar. 

Podemos entonces definir el test $\phi^*$ como el test que tiene el conjunto $R^*$ como región de rechazo, es decir, 
\begin{equation}
	\phi^*(x) = 1 \Leftrightarrow x\in R^*.
\end{equation}

Finalmente, determinaremos la constante $k$ de tal manera de que 
\begin{equation}
	\alpha_{\phi^\star}(\theta_0) = \mathbb{P}_{\theta_0}(x\in R^*) = \alpha,\ \alpha\in[0,1], 
\end{equation}
donde, por definición, $\phi^\star\in T_\alpha$. Consecuentemente, de acuerdo al siguiente lema, $\phi^\star$ es el test UMP en $T_\alpha$.

\begin{lemma}[Neyman-Pearson]
	Consideremos un test de hipótesis de la forma 
	\begin{equation}
		H_0:\theta =\theta_0\quad \text{v.s.}\quad H_1:\theta = \theta_1
	\end{equation}
	con probabilidad de rechazo dada por
	\begin{equation}
			\alpha_\phi(\theta) = \mathbb{P}(p_1\geq k p_0).
		\end{equation}
	Entonces, el test definido arriba que rechaza $H_0$ si $p_1\geq k p_0$ con $\alpha_\phi(\theta) = \alpha,\theta\in\Theta_0$ es el UMP.
\end{lemma}

\begin{proof}
	Consideremos un test $\phi\in T_\alpha$ con región crítica dada por $R$. Recordemos que  la probabilidad de los datos estén en la región R es 
	\begin{equation}
		\Probt{R} = \int_R p_\theta(x)\dx.
	\end{equation}
	Luego, podemos escribir 
	\begin{align}
		\Probt{R} &= \Probt{R\cap R^*} + \Probt{R\cap \bar{R}^*}\\
		\Probt{R^*} &= \Probt{R^*\cap R} + \Probt{R^*\cap \bar{R}},
	\end{align}
	restando y evaluando para $\theta=\theta_1$, tenemos
	\begin{align}
		\Probtu{R^*} - \Probtu{R} 	&=  \Probtu{R^*\cap \bar{R}} - \Probtu{R\cap \bar{R}^*}\nonumber \\
									&=  \int_{R^*\cap \bar{R}} p_{\theta_1}(x)\dx - \int_{R\cap \bar{R}^*} p_{\theta_1}(x)\dx \nonumber\\
									&\geq  k\int_{R^*\cap \bar{R}} p_{\theta_0}(x)\dx - k\int_{R\cap \bar{R}^*} p_{\theta_0}(x)\dx \quad[\text{pues } p_1\geq k p_0 \text{ en } R^*]\nonumber\\
									&= k\left( \Probtz{R^*\cap \bar{R}} - \Probtz{R\cap \bar{R}^*}\right)\nonumber \\
									&= k\left( \underbrace{\Probtz{R^*}}_{=\alpha} - \underbrace{\Probtz{R}}_{\leq\alpha}\right) \quad[\text{primera igualdad de este desarrollo}]\nonumber\\
									&\geq 0
	\end{align}
	Hemos probado que $\Probtu{R^*} \geq \Probtu{R}$, es decir, si $\theta = \theta_1$ entonces $R^*$ tiene mayor probabilidad que cualquier otra región, es es decir, el test que tiene a $R^*$ por región es el test UMP.

	
\end{proof}

\clase{clase 16: 3/10}

\begin{example}
	Sea $X_1,\ldots, X_n$ iid $\ber{\theta}, \theta\in\{\theta_0,\theta_1\}$: 
	\begin{equation}
		H_0:\theta =\theta_0\quad \text{v.s.}\quad H_1:\theta = \theta_1.
	\end{equation} 
	Asumamos que $\theta_1>\theta_0$ y expresemos las densidades de cada hipótesis como 
	\begin{equation}
		p_i(x) = \theta_i^{\sum x_j}(1-\theta_i)^{n-\sum x_j},\ i=0,1.
	\end{equation}
	Para rechazar $H_0$ según el test de Neyman-Pearson, es decir, $x\in R^*$ de acuerdo a la ecuación \eqref{eq:R_NR}, el test requiere: 
	\begin{equation}
		\label{eq:ejemplo:NR}
		\frac{p_1(x)}{p_0(x)} = \frac{\theta_1^{\sum x_j}(1-\theta_1)^{n-\sum x_j}}{\theta_0^{\sum x_j}(1-\theta_0)^{n-\sum x_j}} = \left(\frac{1-\theta_1}{1-\theta_0}\right)^n\left(\frac{\theta_1(1-\theta_0)}{\theta_0(1-\theta_1)}\right)^{\sum x_j}\geq k,
	\end{equation}
	donde, usando el hecho de que $\theta_1\geq\theta_0$, podemos justificar que la expresión anterior es monótona en  $\sum x_j$, consecuentemente, $\sum x_j$ debe ser lo suficientemente grande para rechazar $H_0$.

	Ahora, para calcular el valor de $k$ dado un $\alpha$, tenemos que resolver $\Probtz{x\in R^*} = \alpha$. Primero notemos que la ecuación \eqref{eq:ejemplo:NR} es equivalente a 

	\begin{align}
	 	\left(\frac{1-\theta_1}{1-\theta_0}\right)^n\left(\frac{\theta_1(1-\theta_0)}{\theta_0(1-\theta_1)}\right)^{\sum x_j}\geq k 
	 	& \Leftrightarrow 
	 	\left(\frac{\theta_1(1-\theta_0)}{\theta_0(1-\theta_1)}\right)^{\sum x_j}\geq k \left(\frac{1-\theta_0}{1-\theta_1}\right)^n\\
	 	& \Leftrightarrow 
	 	\sum x_j \loga{\frac{\theta_1(1-\theta_0)}{\theta_0(1-\theta_1)}}\geq n \loga{k \left(\frac{1-\theta_0}{1-\theta_1}\right)} \nonumber\\
	 	& \Leftrightarrow 
	 	\sum x_j \geq \frac{n \loga{k \left(\frac{1-\theta_0}{1-\theta_1}\right)}}{\loga{\frac{\theta_1(1-\theta_0)}{\theta_0(1-\theta_1)}}} = k' \nonumber
	 \end{align} 
	 como $\sum x_j$ es binomial, podemos resolver directamente para $k'$ (y consecuentemente para $k$).
\end{example}


\section{Test de Wald} 
\label{sub:test_de_wald}

Este test nos permite evaluar si un parámetro $\theta$ toma no un valor $\theta_0$ dado. Consideremos un parámetro escalar y $\thetahat$ un estimador asintóticamente normal, es decir, 
\begin{equation}
	\frac{\thetahat - \theta_0}{\ee} \sim \cN(0,1),
\end{equation}
cuando el número de observaciones tiende a infinito y $\ee=\sqrt{\V{\thetahat}}$ es conocido como el \emph{error estándar} y puede ser calculado muestralmente o con respecto a $\theta_0$. Entonces, el test de Wald de tamaño $\alpha$ para las hipótesis 
	\begin{equation}
		H_0:\theta =\theta_0\quad \text{v.s.}\quad H_1:\theta \neq \theta_0
	\end{equation}
indica rechazar $H_0$ cuando el pivote $W=\frac{\thetahat - \theta_0}{\ee}$ cumple con 
\begin{equation}
	|W|\geq z_{\alpha/2},
\end{equation}
donde $z_{\alpha/2}=\Phi(1-\alpha/2)$, es decir, $\Prob{Z\geq z_{\alpha/2}}=\alpha/2,\ Z\sim\cN(0,1)$. 

\begin{remark}
	Notemos que, asintóticamente, el tamaño del test de Wald de tamaño  $\alpha$, es $\alpha$. En efecto, 
\begin{equation}
	\Probtz{|W|\geq z_{\alpha/2}} = \Probtz{\left|\frac{\thetahat - \theta_0}{\ee}\right|\geq z_{\alpha/2}} \rightarrow  = \Probtz{|Z|\geq z_{\alpha/2}} = \alpha
\end{equation}
donde, hemos usado que $Z\sim\cN(0,1)$.
\end{remark}


\begin{example}
 Consideremos dos conjuntos de observaciones $X_1,\ldots,X_n$ y $Y_1,\ldots,Y_n$, con medias respectivas $\mu_1$ y $\mu_2$. Se requiere evaluar las hipótesis
 	\begin{equation}
		H_0:\mu_x =\mu_y \quad \text{v.s.}\quad H_1:\mu_x \neq \mu_y,
	\end{equation}
	lo cual está dentro del alcance del test de Wald denotando $\delta = \mu_x - \mu_y$ e identificando las hipótesis
	 	\begin{equation}
		H_0:\delta =0 \quad \text{v.s.}\quad H_1:\delta \neq 0.
	\end{equation}
Utilicemos el estimador no-paramétrico `plug in' de $\delta$ dado por  $\hat{\delta} = \bar{X}-\bar{Y} = \frac{1}{n}\sum_{i=1}^n x_i + \frac{1}{m}\sum_{i=1}^m y_i$, Además, la varianza de este estimador está dada por $v = \frac{1}{n}s_x^2 + \frac{1}{m}s_y^2$, con lo que la condición sobre el  estadístico de Wald indica que si 
\begin{equation}
	W = \frac{\hat{\delta}-0}{\ee} = \frac{\bar{X}-\bar{Y}}{\sqrt{\frac{1}{n}s_x^2 + \frac{1}{m}s_y^2}} \geq z_{\alpha/2},
\end{equation}
rechazamos $H_0$. 
\end{example}

\begin{remark}
Notemos que el test de Wald de tamaño $\alpha$ rechaza $H_0:\theta=\theta_0$ (v.s. $H_1:\theta\neq\theta_0$) si y solo si 
\begin{equation}
	\theta_0 \notin (\thetahat - \ee z_{\alpha/2},\thetahat + \ee z_{\alpha/2}),
\end{equation}
es decir, realizar el test de Wald es equivalente a calcular el $\alpha$ intervalo de confianza para el parámetro $\theta_0$.
\end{remark}


\section{Test de razón de verosimilitud} 
\label{sub:test_de_RV}

Consideremos un caso más general que los anteriores, donde al menos una de las hipótesis es compuesta, es decir, especifican que el parámetro pertenece a un conjunto en vez de tomar un valor puntual. Es decir, 
\begin{equation}
		H_0:\theta \in\Theta_0\quad \text{v.s.}\quad H_1:\theta \notin\Theta_0.
	\end{equation}
El test de razón de verosimilitud (TRV) indica que se debe rechazar $H_0$ si 
\begin{equation}
	\label{eq:lambda_TRV}
	\lambda(x_1,\ldots,x_n) = \frac{\sup_{\theta\in\Theta_0}L(\theta)}{\sup_{\theta\in\Theta}L(\theta)}
							= \frac{L(\thetahat_0)}{L(\thetahat)}\leq C,
\end{equation}
donde $\thetahat$ es el EMV y $\thetahat_0$ es el EMV restringido a $\{\theta\in\Theta_0\}$. Notemos que la región de rechazo está dada por 
\begin{equation}
	R^* = \{x\in\cX|\lambda(x)\leq C\}.
\end{equation}
\begin{remark}
	Para el caso de hipótesis simples, es decir, $\Theta = \{\theta_0,\theta_1\}$ y $\Theta_0 = \{\theta_0\}$, entonces el TRV coincide con el test de Neyman-Pearson. 
\end{remark}
Al igual que en el TNP, fijamos $C$ en función del un nivel deseado $\alpha$. 

\begin{remark}
	Notemos que podemos escribir la expresión en la ecuación \eqref{eq:lambda_TRV} como 
	\begin{equation}
		\lambda(x_1,\ldots,x_n) =  \ind_{\thetahat\in\Theta_0} + \ind_{\thetahat\in\Theta_1} \frac{\sup_{\theta\in\Theta_0}L(\theta)}{\sup_{\theta\in\Theta_1}L(\theta)}
	\end{equation}
	donde el segundo término (de activarse) es estrictamente menor que 1, con lo que el TRV puede enunciarse en función del estadístico 
	\begin{equation}
		\tilde{\lambda}(x_1,\ldots,x_n) = \frac{\sup_{\theta\in\Theta_0}L(\theta)}{\sup_{\theta\in\Theta_1}L(\theta)} \leq \tilde{k}.
	\end{equation}
\end{remark}


\begin{example}[TRV Bernoulli]
	Sea $X_1,\ldots,X_n\sim\ber{\theta}$ iid, se quiere resolver
	\begin{equation}
		H_0:\theta \leq\theta_0\quad \text{v.s.}\quad H_1:\theta >\theta_0,
	\end{equation}
donde $\theta_0$ es conocido y sabemos que $p_\theta(x) = \theta^{n\bar{x}}(1-\theta)^{n(1-\bar{x})}$. En la notación de la definición anterior del TRV, podemos identificar
\begin{equation}
	\Theta_0 = [0,\theta_0] \quad \& \quad \Theta_1 = (\theta_0, 1]
\end{equation}
calculamos el EMV (restricto e irrestricto) mediante 
\begin{align}
	\thetahat &= \hat{x}\quad \text{irrestricto}\\
	\thetahat_0 &= \hat{x} \quad \text{si } \hat{x}\in\Theta_0,\ \theta_0 \text{ si no.}
\end{align}
podemos escribir esta última expresión como $\thetahat_0 = \hat{x} \ind_{\hat{x}\in\Theta_0} + \theta_0\ind_{\hat{x}\notin\Theta_0}$, entonces

\begin{align}
	\lambda(x) &= \frac{L(\thetahat_0)}{L(\thetahat)}\\ 
				&= \frac{L(\hat{x})}{L(\hat{x})} \ind_{\hat{x}\in\Theta_0} + \frac{L(\theta_0)}{L(\hat{x})} \ind_{\hat{x}\notin\Theta_0} \\
				&= \ind_{\hat{x}\in\Theta_0} + \ind_{\hat{x}\notin\Theta_0} \left(\frac{\theta_0}{\hat{x}}\right)^{n\bar{x}}\left(\frac{1-\theta_0}{1-\hat{x}}\right)^{n(1-\bar{x})}
\end{align}
\todo[inline]{graficar $\lambda(x)$ (en función de $\bar{x}$)}
Donde ahora rechazaremos si $\lambda(x)\leq C$, pero, ¿cómo elegimos $C$?

Recordemos que queremos que el test sea de nivel $\alpha$, es decir, 
\begin{equation}
 	\sup_{\theta\in\Theta_0} \Probt{\lambda(x)\leq C} = \alpha
 \end{equation} 
 donde recordemos que $\lambda(x)$ es una función decreciente de $\bar{x}$, por lo que la condición $\lambda(x)\leq C$ puede expresarse como $\bar{x}\geq C'$, para algún $C'$. Esta expresión dependerá de $C'$, que es función de $C$, de $\theta_0$ y de $\alpha$; despejamos para $C$.

 \end{example} 
\clase{clase 17: 8/10}

 \section{Test $\chi^2$} 
\label{sub:test_chi2}

Este test es usado para verificar si datos multinomiales siguen una distribución dada. Es decir, denotando $p_0 = (p_{01},\ldots,p_{0k})\in[0,1]^k$ y $n\in\N$, estamos interesados en verificar si la distribución $p$ de una variable multinomial sigue
	\begin{equation}
		H_0:p=p_0\quad \text{v.s.}\quad H_1:p\neq p_0.
	\end{equation}
Para esto consideramos $X_1,\ldots,X_n\sim\bin{X;p,n}$ y construyamos el siguiente estadístico
\begin{equation}
	\label{eq:chi2_stat}
	T = \sum_{i=1}^k \frac{(X_i - n p_{0j})^2}{n p_{0j}} = \sum_{i=1}^k \frac{(X_i - \E{X_j})^2}{\E{X_j}},
\end{equation}
donde las esperanzas $\E{X_j} = np_{0j}$ son tomadas con respecto a $p_0$. Este estadístico representa una medida de discrepancia entre frecuencias observadas ($X_i$) y esperadas ($np_{0j}$).

Bajo $H_0$, la distribución asintótica (es decir, cuando el número de observaciones tiene a infinito) es chi cuadrado con $k-1$ grados de libertad, lo cual denotamos por $\chi^2_{k-1}$. Consecuentemente, el test rechaza la hipótesis nula si $T\geq \chi^2_{k-1,\alpha}$, donde $\Prob{T\geq \chi^2_{k-1,\alpha}} = \alpha$.

\begin{example}[Perritos vagos]
	Consideremos un criadero de perros recogidos de la calle, donde podemos identificar cada individuo según tamaño (grande/chico) y color (blanco/negro). Consecuentemente, existen las categorías \\
	\centerline{\emph{\{grande-negro, grande-blanco, chico-negro, chico-blanco\}}.}
	En base a un estudio preliminar realizado por el GrUpo cAnino Universitario (GUAU), se sabe que estas clases deben distribuir, respectivamente, de acuerdo a \\
	\centerline{\emph{\{9/16, 3/16, 3/16, 1/16\}}.}
	El último censo del criadero, el cual solo pudo recolectar información de 556 ejemplares, arrojó los siguiente valores observados: $X=(315, 101, 108,32)$. Esto permite calcular el estadístico del test $\chi^2$ mediante la ecuación \eqref{eq:chi2_stat} obteniendo
	\begin{equation}
		T_{\chi^2} = 0.47.
	\end{equation}
	Para $\alpha=0.05$, tenemos $\chi^2_{3,\alpha} = 7.815>0.47$, por lo que no existe evidencia suficiente para rechazar $H_0$. Es decir, el censo no contradice el estudio realizado por el GUAU. 
\end{example}

Si bien a primera vista, el test $\chi^2$ parece un tanto simple, pues es \emph{solo} definido para VA multinomiales, es un test del tipo \emph{bondad de ajuste} que puede ser extendido a casos no paramétricos.

\todo[inline]{definir qué es un problema estadístico no paramétrico} 

Asumamos $X_1,\ldots,X_n\sim F$ iid, donde $F$ es una distribución desconocida sobre $\R$ (tanto en forma como en parámetros). Planteamos además el siguiente problema de test de hipótesis
	\begin{equation}
		H_0:F=F_0\quad \text{v.s.}\quad H_1:F\neq F_0,
	\end{equation}
	donde $F_0$ sí es una distribución conocida. 
Podemos aplicar el test $\chi^2$ mediante la discretización de este problema: definimos los intervalos
\begin{equation}
 	I_i = (a_{i-1},a_i],\quad -a_0,a_k = \infty, \quad \{a_i\}_{i=1}^{k-1}\subset\R,
 \end{equation} 
 y definimos las VAs
 \begin{equation}
 	N_i = \sum_{j=1}^n \ind_{x_j\in I_i},
 \end{equation}
 las cuales tienen ley multinomial con parámetro $\theta_i = F(a_i)- F(a_{i-1}),\quad i=1,\ldots,k$. Finalmente, bajo $H_0$ resulta $\theta_i = F_0(a_i)- F_0(a_{i-1}), i=1,\ldots,k$, lo cual nos permite aplicar el test $\chi^2$.


 \section{Test de Kolmogorov-Smirnov} 
\label{sub:test_KS}

Ahora consideramos otro enfoque, basado en una estrategia muy distinta, al problema de test de hipótesis anterior para distribuciones no paramétricas:
	\begin{equation}
		H_0:F=F_0\quad \text{v.s.}\quad H_1:F\neq F_0.
	\end{equation}
En vez de discretizar, podemos construir la distribución empírica dada por 
\begin{equation}
	F_n(x) = \frac{1}{n}\sum_{i=1}^n\ind_{x_j\leq x}, 
\end{equation}
la cual realmente es una distribución (discontinua). 

Sabemos que, debido a la ley de los grandes números, 
\begin{equation}
	F_n(x)\to \E{\ind_{X\leq x}} = \Prob{X\leq x} =  F(x),
\end{equation}
además, por el teorema de Glivenko-Cantelli, tenemos 
\begin{equation}
	\sup_x|F_n(x)-F(x)| \to 0 \quad\text{c.s.}
\end{equation}
Lo anterior nos permite definir el estadístico  $D_n = \sup_x|F_n(x)-F_0(x)|$ y la región crítica
\begin{equation}
	R = \{x|D_n\geq k_\alpha\},
\end{equation}
donde $k_\alpha$ se elige imponiendo $\Probtz{D_n\geq  k_\alpha}=\alpha$.

\begin{remark}
El test de  Kolmogorov-Smirnoff sirve tanto para verificar si una VA sigue una distribución dada o si bien dos distribuciones siguen la misma distribución (desconocida).
\end{remark}

 \section{Test de Wilcoxon} 
\label{sub:test_Wilc}

Este es otro testo no paramétrico para verificar si dos VAs siguen la misma distribución. Consideremos las observaciones 
\begin{equation}
	X_1,\ldots,X_n\sim F,\quad Y_1,\ldots,Y_n\sim G,
\end{equation}
donde $F$ y $G$ son dos distribuciones, de las cuales solo sabemos que son continuas. 

Consideremos el siguiente problema de test de hipótesis: 
	\begin{equation}
		H_0:F=G\quad \text{v.s.}\quad H_1:F\neq G.
	\end{equation}
El test de Wilcoxon se enfoca en este escenario pero solo es sensible a diferentes \emph{localizaciones}, es decir, si $G$ es una versión desplazada de $F$.

Antes de ver el test de Wilcoxon, notemos que si nos interesase detectar estas desviaciones, entonces podríamos considerar un test que rechace $H_0$ si $|\bar{X} - \bar{Y}|\geq K$. Esto  es exactamente lo que hace el TRV en el problema 
	\begin{equation}
		H_0:\mu =  \eta \quad \text{v.s.}\quad H_1:\mu\neq \eta,
	\end{equation}
cuando $X\sim\cN(\mu,\sigma^2)$,  $Y\sim\cN(\eta,\sigma^2)$.

Sin embargo, en el caso general (cuando no sabemos nada de $F$) obtener la ley de $|\bar{X} - \bar{Y}|$ bajo $H_0$ no es trivial, lo cual es necesario para $\Probtz{|\bar{X} - \bar{Y}|\geq K} = \alpha$. En esta situación, el test de Wilcoxon propone considerar la siguiente observación conjunta 
\begin{equation}
	(z_1,\ldots,z_{m+n}) = (x_1,\ldots,x_n,y_1,\ldots,y_m),
\end{equation}
para luego considerar la secuencia ordenada de valores $z_i$ dados por 
\begin{equation}
	\min_i\{z_i\} = z_{(1)}\leq z_{(2)}\leq\cdots\leq z_{(n+m)} = \max_i\{z_i\}.
\end{equation}

Ahora podemos definir el concepto de \emph{rango} como la posición en el orden anterior, es decir, donde el rango de $z_{(1)}$ es 1, el rango de $z_{(2)}$ es dos  y así sucesivamente. 

\todo[inline]{dibujo de bolas negras y blancas.}

Denotando el rango de $x_i$ como $R_i$, podemos construir el estadístico
\begin{equation}
 	W = \sum_{i=1}^{n}R_i,
 \end{equation} 
esta cantidad debe intuitivamente interpretarse como el promedio de los rangos (es decir de la posiciones) que toman las observaciones de la variable $X$, por rechazamos $H_0$ si $W$ es muy pequeño o muy grande, es decir, si las muestras de $X$ no quedan \emph{mezcladas} con las de $Y$. 

 Esto es posible por que la distribución de $W$ bajo $H_0$ puede ser calculada y de hecho no depende de $F$, esto es porque (bajo $H_0$) los elementos de $z_i$ son iid, con lo que todas las posible permutaciones del los valores $z_i$ tienen la misma probabilidad dada por $\binom{n+m}{n}$.

 \begin{remark}[¿Cómo obtenemos la región crítica $R$ para este test?]
 	Podemos proceder de forma iterativa: Asumimos $H_0$, consideramos $R=\emptyset$ y agregamos las configuraciones de bolitas que tienen el menor y mayor valor de $W$, luego seguimos con las siguientes configuraciones hasta acumular una probabilidad $\Probtz{W\in\R} = \alpha$.
 \end{remark}

