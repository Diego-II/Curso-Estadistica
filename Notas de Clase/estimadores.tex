%!TEX root = apunte_estadistica.tex


\chapter{Estimadores}

Consideremos una función del parámetro de una familia paramétrica $\familiaparametrica$, $g(\theta)$.  Un estimador puntual de $g(\theta)$ es un estadístico, es decir, una función de la VA $X$, que toma valores en el mismo conjunto que $g(\Theta)$. En general denotaremos como $\gh(X)$ el estimador de $g(\theta)$ aplicado a $X$ 


\begin{example}[Estimador de la media Gaussiana]
	\label{ex:estimador_media}
	Consideremos $X = (X_1,\ldots,X_n)\sim\cN(\mu,\sigma^2)$. Un estimador de $g(\theta) = g(\mu,\sigma) = \mu$ es el estadístico 
	\begin{equation}
		\gh(X) = \frac{1}{n}\sum_{i=1}^nX_i
	\end{equation} 
\end{example}

Una clase muy importante de estimadores son los estimadores insesgados. 

\begin{definition}[Estimador insesgado]
	Sea $\ghx$ un estimador de $g(\theta)$. Este estimador es insesgado si 
	\begin{equation}
		\E{\ghx} = g(\theta)
	\end{equation}
	y el sesgo de $\gh$ se define como 
	\begin{equation}
		b_\gh(\theta) = \E{\ghx} - g(\theta)
	\end{equation}
\end{definition}



Los estimadores insesgados juegan un rol importante en el estudio y aplicación de la estadística, sin embargo, uno no siempre debe poner exclusiva atención a ellos. Los siguiente ejemplos ilustran el rol del estimador insesgado en dos familias paramétricas distintas. 

\begin{example}[Estimador insesgado de la media Gaussiana]
	\label{ex:estimador_in_media}
	El estimador de $g(\theta) =  \mu$ descrito en el Ejemplo \ref{ex:estimador_media} es insesgado, en efecto: 
	\begin{equation}
		\E{\ghx} = \E{\frac{1}{n}\sum_{i=1}^nX_i}	= \frac{1}{n}\sum_{i=1}^n\E{X_i}		= \frac{1}{n}\sum_{i=1}^n\mu = \mu	
	\end{equation}
\end{example}


\begin{example}[Estimador de la taza de la distribución exponencial\footnote{Schervish}]
	\label{ex:estimador_exponancial}
	Consideremos $X\sim Exp(\theta)$, donde $Exp(x|\theta) = \theta\exp(-\theta x)$, y asumamos que existe un estimador insesgado $\ghX$ de $g(\theta) = \theta$, entonces, 
	\begin{equation}
		\E{\ghX} = \int_0^\infty \ghx\theta\exp(-\theta x)\d x = \theta, \forall \theta,
	\end{equation}
	lo cual es equivalente a decir que $\int_0^\infty \ghx\exp(-\theta x)\d x = 1, \forall \theta$ o bien que (al derivar ambos lados de esta expresión c.r.a. $\theta$)  $\int_0^\infty x\ghx\exp(-\theta x)\d x = 0, \forall \theta$.

	Esta última expresión implica que $\E{X\ghX} = 0$, lo que a su vez y considerando que $X$ es un estadístico suficiente y completo, implica que $\ghX=0$ c.s. $\forall \theta$. Como esto contradice el hecho de que $\ghX$ es insesgado, no es posible construir estimadores insesgados para $\theta$ en la distribución exponencial.
\end{example}


Veamos ahora un ejemplo de un estimador sesgado de la varianza y como se puede construir un estimador insesgado. 

\begin{example}
Consideremos una familia paramétrica $\familiaparametrica$ y denotemos por $\mu$ y $\sigma^2$ su media y su varianza respectivamente. Usando las observaciones $x_1,x_2,\ldots,x_n$, calculemos la varianza del estimador de la media, dado por $\xb = \frac{1}{n}\sum_{i=1}^n x_i$ mediante
\begin{equation}
	\label{eq:varianza_media_muestral}
 	\Vt{\xb} = \Vt{\frac{1}{n}	\sum_{i=1}^n x_i}  \underbrace{=}_{\text{i.i.d.}}  \frac{1}{n^2}	\sum_{i=1}^n\Vt{ x_i} =\frac{\sigma^2}{n}
 \end{equation} 
 es decir, el estimador de la media usando $n$ muestras, tiene una varianza $\sigma^2/n$.

 Consideremos ahora el siguiente estimador para la varianza: 
\begin{equation}
	\label{eq:est_varianza_sesgado}
	S_2 = \frac{1}{n}\sum_{i=1}^n (x_i-\xb)^2
\end{equation}
y notemos que la esperanza de dicho estimador es
\begin{align}
	\label{eq:sesgo_varianza}
	\Et{S_2 } &= \Et{\frac{1}{n}\sum_{i=1}^n (x_i-\mu+\mu-\xb)^2}\nonumber\\
				&= \Et{ \frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2 + 2\frac{1}{n}\sum_{i=1}^n(x_i-\mu)(\mu-\xb) + \frac{1}{n}\sum_{i=1}^n(\mu-\xb)^2}\nonumber\\
				&= \Et{ \frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2 - 2(\mu-\xb)^2 + (\mu-\xb)^2}\nonumber\\
				&= \Et{ \frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2 - (\mu-\xb)^2}\nonumber\\
				&= \Vt{x_i} - \Vt{\xb}\quad\text{ver ecuación \eqref{eq:varianza_media_muestral}}\nonumber\\
				&= 	\sigma^2 + \sigma^2/n = \left(\frac{n+1}{n}\right)\sigma^2
\end{align}
Esto quiere decir que el sesgo del estimado en la ecuación \eqref{eq:est_varianza_sesgado} es asintóticamente insesgado, es decir, que su sesgo tiende a cero cuando el número de muestas $n$ tiende a infinito. Sin embargo, notemos que podemos corregir el estimado de la varianza multiplicando el estimador original, $S_2$ en la ecuación \eqref{eq:est_varianza_sesgado} por $n/(n+1)$, con lo que el estimador corregido denotado por 
\begin{equation}
	\label{eq:est_varianza_insesgado}
	S'_2 = \frac{n}{n+1}S_2 =  \frac{1}{n+1}\sum_{i=1}^n (x_i-\xb)^2
\end{equation}
cumple con
\begin{equation}
	\Et{S'_2 } =  \left(\frac{n}{n+1}\right)\Et{S_2} \underbrace{=}_{\text{ec.}\eqref{eq:sesgo_varianza}} \left(\frac{n}{n+1}\right) \left(\frac{n+1}{n}\right)\sigma^2 = \sigma^2
\end{equation}
es decir, el estimador $S'_2$ en la ecuación \eqref{eq:est_varianza_insesgado} es insesgado.
\end{example}




Es natural evaluar la bondad de distintos estimadores (sesgados o insesgados), una forma de hacer esto es definir una función de \textit{pérdida} o \textit{costo} que compara el valor reportado por el estimado y el valor real del parámetro. Luego, como el estimador es una VA, podemos calcular la esperanza de la función de pérdida, lo cual conocemos como riesgo. En general, el costo más frecuentemente utilizado es el costo cuadrático, el cual, de acuerdo a lo recién explicado, tiene el siguiente riesgo: 

\begin{alignat*}{3}
 	R(\theta, \hat{g})  &= \E{(g(\theta) - \ghX)^2}\\
 						& = \E{\left(g(\theta) - \bar{g}+ \bar{g} -\ghX\right)^2}; \quad \bar{g} = \E{ \ghX}\\
 						& = \E{(g(\theta) - \bar{g})^2+2(g(\theta) - \bar{g})\cancel{(\bar{g} -\ghX)} +  (\bar{g} -\ghX)^2}\\
 						& = \underbrace{(g(\theta) - \bar{g})^2}_{=b_{\gh}^2\ (\text{sesgo}^2)} +  \underbrace{\E{(\bar{g} -\ghX)^2}}_{=V_{\gh}\ \text{(varianza)}}
 \end{alignat*} 

 Consecuentemente, si un estimador es insesgado el riesgo es simplemente su varianza, lo cual motiva la siguiente definición de optimalidiad para estimadores insesgados. 

 \begin{definition}[Estimador insesgado de varianza uniformemente mínima]
  	El estimador $\gh(x)$ de $g(\theta)$ es un estimador insesgado de varianza uniformemente mínima (EIVUM) si es insesgado y si $\forall \gh^*:\cX\rightarrow g(\Theta)$ estimador insesgado se tiene
  	\begin{equation}
  		\Vt{\gh}\leq\Vt{\gh^*}, \forall \theta\in\Theta.
  	\end{equation}
  \end{definition} 

  \red{teorema de rao blackwell}
